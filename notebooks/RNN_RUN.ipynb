{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1JP3xG1lgaT",
        "outputId": "e5806f8d-a4e3-4e74-f443-f2c88874fd84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import accelerate\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvAKJiG0nFJe",
        "outputId": "cb9922e8-9e5f-41dc-d930-0622e4a2d521"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_dataset = CommentDataset(train_df['sequences'].tolist(), train_df['mapped_labels'].tolist())\n",
        "test_dataset = CommentDataset(test_df['sequences'].tolist(), test_df['mapped_labels'].tolist())\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "RYUXonYmnGjY",
        "outputId": "be31181c-6d3e-4182-8d61-1c9a8a101f62"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'CommentDataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-965dad9977d4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Splitting the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCommentDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sequences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mapped_labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCommentDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sequences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mapped_labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CommentDataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/thelinkof/cleaned_movie_data_comment.csv')\n",
        "\n",
        "# Data cleaning function\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'<.*?>', '', str(text))\n",
        "    text = emoji.replace_emoji(text, replace='')\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower().strip()\n",
        "    return text\n",
        "\n",
        "df['cleaned_comment'] = df['comment_text'].apply(clean_text)\n",
        "\n",
        "# Tokenization and Stop-Word Removal\n",
        "stop_words = set(stopwords.words('turkish'))\n",
        "def tokenize_and_remove_stop_words(text):\n",
        "    tokens = word_tokenize(text, language='turkish')\n",
        "    tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
        "    return tokens\n",
        "\n",
        "df['tokens'] = df['cleaned_comment'].apply(tokenize_and_remove_stop_words)\n",
        "\n",
        "# Convert tokens to sequences\n",
        "def tokens_to_sequence(tokens, vocab, max_len):\n",
        "    sequence = [vocab.get(token, 1) for token in tokens]  # 1 is the index for unknown words\n",
        "    if len(sequence) < max_len:\n",
        "        sequence += [0] * (max_len - len(sequence))  # Padding\n",
        "    return sequence[:max_len]\n",
        "\n",
        "# Building vocabulary from tokens\n",
        "all_tokens = [token for sublist in df['tokens'].tolist() for token in sublist]\n",
        "vocab = {token: idx + 2 for idx, token in enumerate(set(all_tokens))}\n",
        "vocab['<PAD>'] = 0  # Padding\n",
        "vocab['<UNK>'] = 1  # Unknown words\n",
        "\n",
        "# Convert all tokens to sequences\n",
        "max_len = 512\n",
        "df['sequences'] = df['tokens'].apply(lambda x: tokens_to_sequence(x, vocab, max_len))\n",
        "\n",
        "# Remap the ratings to 0, 1, 2\n",
        "df['mapped_labels'] = df['comment_rating'].apply(lambda x: 2 if x >= 7 else (1 if x >= 4 else 0))\n",
        "print(\"Unique labels after mapping:\", df['mapped_labels'].unique())\n",
        "\n",
        "# Creating the dataset\n",
        "class CommentDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sequence = torch.tensor(self.sequences[index], dtype=torch.long)\n",
        "        label = torch.tensor(self.labels[index], dtype=torch.long)\n",
        "        return sequence, label\n",
        "\n",
        "# Splitting the dataset\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_dataset = CommentDataset(train_df['sequences'].tolist(), train_df['mapped_labels'].tolist())\n",
        "test_dataset = CommentDataset(test_df['sequences'].tolist(), test_df['mapped_labels'].tolist())\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# Define the RNN model\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, output_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        out = self.dropout(lstm_out[:, -1, :])  # Take the output of the last timestep\n",
        "        return self.linear(out)\n",
        "\n",
        "# Initialize the model\n",
        "output_size = 3  # Assuming 3 classes\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "vocab_size = len(vocab)  # Number of unique tokens in vocab\n",
        "model = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers).to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size for training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # Define how to compute loss in custom Trainer\n",
        "        inputs, labels = inputs['input_ids'], inputs['labels']\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "train_features = [{'input_ids': inputs, 'labels': labels} for inputs, labels in train_loader]\n",
        "eval_features = [{'input_ids': inputs, 'labels': labels} for inputs, labels in test_loader]\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_features,        # training dataset\n",
        "    eval_dataset=eval_features           # evaluation dataset\n",
        ")\n",
        "\n",
        "# Train and evaluate the model\n",
        "trainer.train()\n",
        "eval_result = trainer.evaluate()\n",
        "print(f\"Results: {eval_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "R0QvLoCJm4CP",
        "outputId": "df55611b-6691-44e9-9a0c-115c36f06fa2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Unique labels after mapping: [2 1 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c8aadf0a98c4>\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# Define the training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./results'\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# output directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m              \u001b[0;31m# number of training epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length,...\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1639\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_greater_or_equal_than_2_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"mlu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36mdevice\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \"\"\"\n\u001b[1;32m   2148\u001b[0m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_devices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m_setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2053\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2054\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2055\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m   2056\u001b[0m                     \u001b[0;34mf\"Using the `Trainer` with `PyTorch` requires `accelerate>={ACCELERATE_MIN_VERSION}`: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2057\u001b[0m                     \u001b[0;34m\"Please run `pip install transformers[torch]` or `pip install accelerate -U`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/thelinkof/cleaned_movie_data_comment.csv')\n",
        "\n",
        "# Clean text data\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = emoji.replace_emoji(text, replace='')\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', '', text)\n",
        "    text = text.lower().strip()\n",
        "    return text\n",
        "\n",
        "df['cleaned_comment'] = df['comment_text'].apply(clean_text)\n",
        "\n",
        "# Tokenize using RNN-based tokenizer\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(df['cleaned_comment'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df['cleaned_comment'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=512)\n",
        "\n",
        "# One-hot encode labels\n",
        "labels = df['comment_rating'].apply(lambda x: 2 if x >= 7 else (1 if x >= 4 else 0))\n",
        "labels = to_categorical(labels)\n",
        "\n",
        "# Split data\n",
        "train_sequences, test_sequences, train_labels, test_labels = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=128, input_length=512))\n",
        "model.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(train_sequences, train_labels, epochs=3, batch_size=16, validation_data=(test_sequences, test_labels))\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(test_sequences, test_labels)\n",
        "print(f\"Results: Loss={loss:.3f}, Accuracy={accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufgqE4FJl5Z0",
        "outputId": "9706d69a-4afe-4492-d404-79121bd3dbfe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1182/1182 [==============================] - 2020s 2s/step - loss: 0.4057 - accuracy: 0.8897 - val_loss: 0.4390 - val_accuracy: 0.8749\n",
            "Epoch 2/3\n",
            "1182/1182 [==============================] - 1943s 2s/step - loss: 0.3363 - accuracy: 0.9110 - val_loss: 0.4631 - val_accuracy: 0.8665\n",
            "Epoch 3/3\n",
            "1182/1182 [==============================] - 1955s 2s/step - loss: 0.3040 - accuracy: 0.9200 - val_loss: 0.4725 - val_accuracy: 0.8451\n",
            "148/148 [==============================] - 20s 132ms/step - loss: 0.4725 - accuracy: 0.8451\n",
            "Results: Loss=0.472, Accuracy=0.845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#total one:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!pip install torch pandas nltk emoji transformers\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/thelinkof/cleaned_movie_data_comment.csv')\n",
        "\n",
        "# Data cleaning function\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'<.*?>', '', str(text))\n",
        "    text = emoji.replace_emoji(text, replace='')\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower().strip()\n",
        "    return text\n",
        "\n",
        "df['cleaned_comment'] = df['comment_text'].apply(clean_text)\n",
        "\n",
        "# Tokenization and Stop-Word Removal\n",
        "stop_words = set(stopwords.words('turkish'))\n",
        "def tokenize_and_remove_stop_words(text):\n",
        "    tokens = word_tokenize(text, language='turkish')\n",
        "    tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
        "    return tokens\n",
        "\n",
        "df['tokens'] = df['cleaned_comment'].apply(tokenize_and_remove_stop_words)\n",
        "\n",
        "# Convert tokens to sequences\n",
        "def tokens_to_sequence(tokens, vocab, max_len):\n",
        "    sequence = [vocab.get(token, 1) for token in tokens]  # 1 is the index for unknown words\n",
        "    if len(sequence) < max_len:\n",
        "        sequence += [0] * (max_len - len(sequence))  # Padding\n",
        "    return sequence[:max_len]\n",
        "\n",
        "# Building vocabulary from tokens\n",
        "all_tokens = [token for sublist in df['tokens'].tolist() for token in sublist]\n",
        "vocab = {token: idx + 2 for idx, token in enumerate(set(all_tokens))}\n",
        "vocab['<PAD>'] = 0  # Padding\n",
        "vocab['<UNK>'] = 1  # Unknown words\n",
        "\n",
        "# Convert all tokens to sequences\n",
        "max_len = 512\n",
        "df['sequences'] = df['tokens'].apply(lambda x: tokens_to_sequence(x, vocab, max_len))\n",
        "\n",
        "# Remap the ratings to 0, 1, 2\n",
        "df['mapped_labels'] = df['comment_rating'].apply(lambda x: 2 if x >= 7 else (1 if x >= 4 else 0))\n",
        "print(\"Unique labels after mapping:\", df['mapped_labels'].unique())\n",
        "\n",
        "# Creating the dataset\n",
        "class CommentDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sequence = torch.tensor(self.sequences[index], dtype=torch.long)\n",
        "        label = torch.tensor(self.labels[index], dtype=torch.long)\n",
        "        return sequence, label\n",
        "\n",
        "# Splitting the dataset\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_dataset = CommentDataset(train_df['sequences'].tolist(), train_df['mapped_labels'].tolist())\n",
        "test_dataset = CommentDataset(test_df['sequences'].tolist(), test_df['mapped_labels'].tolist())\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Define the RNN model\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, output_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        out = self.dropout(lstm_out[:, -1, :])  # Take the output of the last timestep\n",
        "        return self.linear(out)\n",
        "\n",
        "# Initialize the model\n",
        "output_size = 3  # Assuming 3 classes\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "vocab_size = len(vocab)  # Number of unique tokens in vocab\n",
        "model = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers).to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size for training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # Define how to compute loss in custom Trainer\n",
        "        inputs, labels = inputs['input_ids'], inputs['labels']\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "train_features = [{'input_ids': inputs, 'labels': labels} for inputs, labels in train_loader]\n",
        "eval_features = [{'input_ids': inputs, 'labels': labels} for inputs, labels in test_loader]\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_features,        # training dataset\n",
        "    eval_dataset=eval_features           # evaluation dataset\n",
        ")\n",
        "\n",
        "# Train and evaluate the model\n",
        "trainer.train()\n",
        "eval_result = trainer.evaluate()\n",
        "print(f\"Results: {eval_result}\")\n"
      ],
      "metadata": {
        "id": "MrGk5wcpm5J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Evaluate function\n",
        "def evaluate(model, test_sequences, test_labels):\n",
        "    predictions = model.predict(test_sequences)\n",
        "    predictions = predictions.argmax(axis=1)\n",
        "    true_labels = test_labels.argmax(axis=1)\n",
        "\n",
        "    return predictions, true_labels\n",
        "\n",
        "# Evaluate the model\n",
        "predictions, true_labels = evaluate(model, test_sequences, test_labels)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "recall = recall_score(true_labels, predictions, average='weighted')\n",
        "precision = precision_score(true_labels, predictions, average='weighted')\n",
        "roc_auc = roc_auc_score(pd.get_dummies(true_labels), pd.get_dummies(predictions), multi_class='ovr')\n",
        "\n",
        "# Plot metrics\n",
        "metrics = {'Accuracy': accuracy, 'F1 Score': f1, 'Recall': recall, 'Precision': precision, 'ROC AUC': roc_auc}\n",
        "metrics_names = list(metrics.keys())\n",
        "metrics_values = list(metrics.values())\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(metrics_names, metrics_values, color=['blue', 'orange', 'green', 'red', 'purple'])\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Model Performance Metrics')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "Tg9FEIhLoTL1",
        "outputId": "15d6b1a6-787c-4e2d-cb46-cbb4e84b15a5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "148/148 [==============================] - 21s 142ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for metric_name, metric_value in metrics.items():\n",
        "    print(f\"{metric_name}: {metric_value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKt5IRH0oV7D",
        "outputId": "9c6e7431-d992-48c8-b284-554d6dd9c4ea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8451\n",
            "F1 Score: 0.8113\n",
            "Recall: 0.8451\n",
            "Precision: 0.7832\n",
            "ROC AUC: 0.5126\n"
          ]
        }
      ]
    }
  ]
}
